{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "debTHUGn8G82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/colab_data/datasets/quora-question-pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gviVBdi8YjI",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MK-pjQW8WVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "ROOT_DIR = '/content/drive/My Drive/colab_data/datasets/quora-question-pairs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgGNlR_f8X7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(os.path.join(ROOT_DIR,'train_preprocessed.csv'))\n",
        "df = df[['question1','question2','is_duplicate']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z5yL7XbPKJF",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch and RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7mJwx6wPbM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjsCCokx85mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(df, test_size=0.2)\n",
        "print(train.shape)\n",
        "print(val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS3TnaEEPYVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuplicateDetector(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, nb_layers=1):\n",
        "      super(DuplicateDetector, self).__init__()\n",
        "      self.vocab_size, self.embedding_dim = pretrained_embedding.shape\n",
        "      self.nb_layers=nb_layers\n",
        "      self.word_embedding = nn.Embedding(\n",
        "            num_embeddings= self.vocab_size,\n",
        "            embedding_dim= self.embedding_dim,\n",
        "            padding_idx=0,\n",
        "            sparse=False\n",
        "        )\n",
        "      self.word_embedding.load_state_dict({'weight': torch.from_numpy(pretrained_embedding)})\n",
        "      self.lstm = nn.LSTM(\n",
        "          input_size = self.embedding_dim,\n",
        "          hidden_size = self.embedding_dim,\n",
        "          num_layers=self.nb_layers,\n",
        "          batch_first=True\n",
        "      )\n",
        "      self.mlc = nn.Linear(self.embedding_dim*2, 1)\n",
        "\n",
        "        \n",
        "    def forward(self, q1, q1_lengths, q2, q2_lengths):\n",
        "      q1_emb = self.word_embedding(q1)\n",
        "      q1_emb = torch.nn.utils.rnn.pack_padded_sequence(q1_emb, q1_lengths, batch_first=True, enforce_sorted=False)\n",
        "      _, q1_hidden_state = self.lstm(q1_emb)\n",
        "      \n",
        "\n",
        "      q2_emb = self.word_embedding(q2)\n",
        "      q2_emb = torch.nn.utils.rnn.pack_padded_sequence(q2_emb, q2_lengths, batch_first=True, enforce_sorted=False)\n",
        "      _, q2_hidden_state = self.lstm(q2_emb)\n",
        "\n",
        "\n",
        "      output = torch.cat((q1_hidden_state[0], q2_hidden_state[0]),0)\n",
        "      output = output.view(-1, self.embedding_dim*2)\n",
        "      output = self.mlc(output)\n",
        "      output = F.sigmoid(output)\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzJdzSaVAUTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print (use_cuda)\n",
        "\n",
        "if use_cuda:\n",
        "  current_device = torch.cuda.current_device()\n",
        "  print(torch.cuda.get_device_name(current_device))\n",
        "else:\n",
        "  current_device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvEPGatQdxFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_weights = np.load('pretrained_emb.npy')\n",
        "emb_weights.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npDimleodb3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =  DuplicateDetector(emb_weights)\n",
        "model.to(current_device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = 100\n",
        "\n",
        "def print_(loss):\n",
        "    print (\"The loss calculated: \", loss)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOPn5Kf29XCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuoraDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    data = df.loc[idx]\n",
        "    q1 = eval(data['question1'])\n",
        "    q1 = torch.LongTensor(q1)\n",
        "    q2 = eval(data['question2'])\n",
        "    q2 = torch.LongTensor(q2)\n",
        "    return {'q1': q1,\n",
        "            'q2': q2,\n",
        "            'label': data['is_duplicate']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0eSkKUHPUQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_collate(batch):\n",
        "  key_data = ['q1', 'q2', 'label']\n",
        "  elem = batch[0]\n",
        "  batch = {key: [d[key] for d in batch] for key in elem if key in key_data}\n",
        "\n",
        "  q1_lens = [len(q1) for q1 in batch['q1']]\n",
        "  q2_lens = [len(q2) for q2 in batch['q2']]\n",
        "\n",
        "  q1_pad = torch.nn.utils.rnn.pad_sequence(batch['q1'], batch_first=True, padding_value=0)\n",
        "  q2_pad = torch.nn.utils.rnn.pad_sequence(batch['q2'], batch_first=True, padding_value=0)\n",
        "\n",
        "  return q1_pad, q2_pad, q1_lens, q2_lens, batch['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkOxlLGmAfEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = QuoraDataset(train)\n",
        "val_dataset = QuoraDataset(val)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=4, collate_fn=pad_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True, num_workers=4, collate_fn=pad_collate)\n",
        "\n",
        "nb_train_batchs = len(train_dataloader)\n",
        "nb_val_batchs = len(val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo--0JBzyNr6",
        "colab_type": "text"
      },
      "source": [
        "# Exercises:\n",
        "\n",
        "1.   Implenent the training component using what you learned from Session 2"
      ]
    }
  ]
}