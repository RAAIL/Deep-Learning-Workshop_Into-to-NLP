{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0 - sentiment analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvpVFFzNtDeC",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset from kaggle (only for google colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJllp69sPbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/kaggle\"\n",
        "%cd /content/drive/My Drive/colab_data/datasets\n",
        "os.mkdir('sentiment-analysis-on-movie-reviews')\n",
        "%cd sentiment-analysis-on-movie-reviews\n",
        "!kaggle competitions download -c sentiment-analysis-on-movie-reviews\n",
        "!ls\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vw1lRrIuJm-",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis Example (Rotten Tomatoes Dataset) <br>\n",
        "Labels:\n",
        "\n",
        "*   0 = negative\n",
        "*   1 = somewhat negative\n",
        "*   2 = neutral\n",
        "*   3 = somewhat positive\n",
        "*   4 = positive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XilqHeJRuIzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "ROOT_DIR = '/content/drive/My Drive/colab_data/datasets/sentiment-analysis-on-movie-reviews'\n",
        "LABELS = ['negative','somewhat negative', 'neutral', 'somewhat positive', 'positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7-mIhRd2OCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adjust plot colors if notebook background is dark\n",
        "default_color = 'white'\n",
        "mpl.rcParams['axes.labelcolor'] = default_color\n",
        "mpl.rcParams['xtick.color'] = default_color\n",
        "mpl.rcParams['ytick.color'] = default_color\n",
        "mpl.rcParams['axes.titlecolor'] = default_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8089TlivMQB",
        "colab_type": "text"
      },
      "source": [
        "### Analyse Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npFywHwCvKc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(os.path.join(ROOT_DIR, 'train.tsv.zip'), sep=\"\\t\")\n",
        "test = pd.read_csv(os.path.join(ROOT_DIR, 'test.tsv.zip'), sep=\"\\t\")\n",
        "sampleSub = pd.read_csv(os.path.join(ROOT_DIR, 'sampleSubmission.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ4gZKCIvLwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKtzXCbsw-m_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-BoOOXvxAKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampleSub.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG9SVQs-0daD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "dist = train['Sentiment'].value_counts()\n",
        "\n",
        "ax.set_xlabel('Sentiment')\n",
        "ax.set_ylabel('Freq')\n",
        "\n",
        "dist.plot(ax=ax, kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNCjOVZNKfw",
        "colab_type": "text"
      },
      "source": [
        "### Exam the word frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ2VkX1u0UC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_wordCloud(label, ax, stopwords=None):\n",
        "  text = ' '.join(phrase for phrase in train[train[\"Sentiment\"] == label]['Phrase'] )\n",
        "  wordcloud = WordCloud(max_words = 100, stopwords = stopwords, background_color=\"black\").generate(text)\n",
        "\n",
        "  ax.imshow(wordcloud, interpolation = 'bilinear', cmap='viridis')\n",
        "  ax.set_title(LABELS[label])\n",
        "  ax.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdyLDaJ6KM1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axs = plt.subplots(3,2,figsize=(24,16))\n",
        "display_wordCloud(0,axs[0][0])\n",
        "display_wordCloud(4,axs[0][1])\n",
        "display_wordCloud(1,axs[1][0])\n",
        "display_wordCloud(3,axs[1][1])\n",
        "display_wordCloud(2,axs[2][0])\n",
        "axs[2][1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqe4xgfPT3KW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update(['movie', 'film', 'one', 'RRB', 'LRB', 'character', 'characters', 'make', 'story'])\n",
        "f, axs = plt.subplots(3,2,figsize=(24,16))\n",
        "display_wordCloud(0,axs[0][0], stopwords)\n",
        "display_wordCloud(4,axs[0][1], stopwords)\n",
        "display_wordCloud(1,axs[1][0], stopwords)\n",
        "display_wordCloud(3,axs[1][1], stopwords)\n",
        "display_wordCloud(2,axs[2][0], stopwords)\n",
        "axs[2][1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7RctTKOVIto",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhSGz2rUPiOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "snowStem=SnowballStemmer('english')\n",
        "\n",
        "from string import punctuation\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPa6FSMOa9Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download packages if not yet downloaded\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggGNg7WFVezH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(phrases):\n",
        "    corpus=[]\n",
        "    for i in range(0,len(phrases)):\n",
        "        phrase=str(phrases[i])\n",
        "        phrase = phrase.lower()\n",
        "        phrase=re.sub('[^a-z]',' ',phrase)\n",
        "        \n",
        "        tokens=[snowStem.stem(w) for w in word_tokenize(phrase)]\n",
        "        phrase=' '.join(tokens)\n",
        "        corpus.append(phrase)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIRFnQmSabO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['phrase_preprocess']=preprocess(train['Phrase'].values)\n",
        "test['phrase_preprocess']=preprocess(test['Phrase'].values)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzaPWstXiBPG",
        "colab_type": "text"
      },
      "source": [
        "# Split Training Set into Training and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hCqc4sriJuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(train[\"phrase_preprocess\"], train['Sentiment'].values, test_size=0.20, random_state=111)\n",
        "x_test = test['phrase_preprocess'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw3ywAjDdpsl",
        "colab_type": "text"
      },
      "source": [
        "# Encoding: Bag-of-Words (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EPQngUxeL6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgArB5E_cBPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_corpus = list(x_train) + list(x_val) + list(x_test)\n",
        "vectorizer = CountVectorizer(min_df = 5)\n",
        "vectorizer.fit(full_corpus)\n",
        "\n",
        "x_train = vectorizer.transform(x_train)\n",
        "x_val = vectorizer.transform(x_val)\n",
        "x_test = vectorizer.transform(x_test)\n",
        "print(x_train[0].shape)\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51OIJSVVkX-S",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oiln_RChm9hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZB3nC6dkeWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, neurons):\n",
        "      super(MLP, self).__init__()\n",
        "      self.neurons = neurons;\n",
        "      self.nb_layers = len(neurons)\n",
        "      self.layers = nn.ModuleList();\n",
        "      self.layers.append(nn.Linear(input_dim, neurons[0]))\n",
        "      \n",
        "      for i in range(1, self.nb_layers):\n",
        "        self.layers.append(nn.Linear(neurons[i-1], neurons[i]))\n",
        "\n",
        "      self.layers.append(nn.Linear(neurons[self.nb_layers-1], 5))\n",
        "        \n",
        "    def forward(self, x):\n",
        "      for i in range(self.nb_layers):\n",
        "        x = F.relu(self.layers[i](x))\n",
        "      x = F.softmax(self.layers[self.nb_layers](x), dim=0)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtiDDu0B1Obx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels =labels\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return {'input': self.features[idx].toarray().flatten(),\n",
        "            'label': self.labels[idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u_0OjftnCCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print (use_cuda)\n",
        "\n",
        "if use_cuda:\n",
        "  current_device = torch.cuda.current_device()\n",
        "  print(torch.cuda.get_device_name(current_device))\n",
        "else:\n",
        "  current_device = torch.device(\"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c3va-FjvFEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLP(x_train.shape[1], [800, 500, 300, 200])\n",
        "model.to(current_device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = 100\n",
        "\n",
        "def print_(loss):\n",
        "    print (\"The loss calculated: \", loss)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_KO-oDKp8Cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = ReviewDataset(x_train, y_train)\n",
        "val_dataset = ReviewDataset(x_val, y_val)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True, num_workers=4)\n",
        "\n",
        "nb_train_batchs = len(train_dataloader)\n",
        "nb_val_batchs = len(val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyErjZXFu4oW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "running_loss = 0.0;\n",
        "with torch.set_grad_enabled(True):\n",
        "  for i_batch, sample_batch in enumerate(train_dataloader):\n",
        "    inputs = sample_batch['input']\n",
        "    labels = sample_batch['label']\n",
        "\n",
        "    inputs = inputs.to(current_device, dtype=torch.float)\n",
        "    labels = labels.to(current_device, dtype=torch.long)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    _y = model(inputs)\n",
        "    loss = loss_fn(_y, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  print_(running_loss/nb_train_batchs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvByz091AvU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "running_loss = 0.0;\n",
        "with torch.set_grad_enabled(False):\n",
        "  for i_batch, sample_batch in enumerate(val_dataloader):\n",
        "    inputs = sample_batch['input']\n",
        "    labels = sample_batch['label']\n",
        "\n",
        "    inputs = inputs.to(current_device, dtype=torch.float)\n",
        "    labels = labels.to(current_device, dtype=torch.long)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    _y = model(inputs)\n",
        "    loss = loss_fn(_y, labels)\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  print_(running_loss/nb_val_batchs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZJOiOynXA4p",
        "colab_type": "text"
      },
      "source": [
        "# Exercises:\n",
        "\n",
        "\n",
        "\n",
        "1.   Try Balancing the dataset and rerun the same models\n",
        "2.   Try Term Frequency-Inverse Document Freuency (tf-idf)\n",
        "3.   Try incorporating additional preprocessing\n",
        "4.   Try adjusting network architecture (i.e. different activation function, # neurons)\n",
        "5.   Try including accuracy evaluation metric\n",
        "6.   Try running for multiple epochs\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}