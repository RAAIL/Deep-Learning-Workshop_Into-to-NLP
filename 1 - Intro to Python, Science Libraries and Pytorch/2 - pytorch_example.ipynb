{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_example.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNewwwgIqrTBbG3ZGLI0r2Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SQJb8MwIO14l","colab_type":"text"},"source":["Pytorch MLP - Ires Dataset ref: https://www.kaggle.com/aaditkapoor1201/iris-classification-pytorch\n"]},{"cell_type":"code","metadata":{"id":"dcvw3mKcOegP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594226690982,"user_tz":240,"elapsed":5805,"user":{"displayName":"Lex Workshop","photoUrl":"","userId":"01577604359645715308"}},"outputId":"4a1b2e60-19ae-4474-eaa8-533266a24a80"},"source":["%matplotlib inline\n","\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.autograd import Variable\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from keras.utils import to_categorical\n","import torch.nn.functional as F"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"G9p_mae1OrdG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1594228281480,"user_tz":240,"elapsed":333,"user":{"displayName":"Lex Workshop","photoUrl":"","userId":"01577604359645715308"}},"outputId":"cbc7fe4d-0239-4175-bbc8-5b9b369968c8"},"source":["dataset = load_iris()\n","# target = to_categorical(dataset.target, num_classes=3)\n","x_train ,x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=42, shuffle=True)\n","print(x_train.shape)\n","print(x_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["(112, 4)\n","(38, 4)\n","(112,)\n","(38,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2OGrBc_3TNDU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594228141265,"user_tz":240,"elapsed":205,"user":{"displayName":"Lex Workshop","photoUrl":"","userId":"01577604359645715308"}}},"source":["class MLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(MLP, self).__init__()\n","        self.layer1 = nn.Linear(input_dim,50)\n","        self.layer2 = nn.Linear(50, 20)\n","        self.layer3 = nn.Linear(20, 3)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        x = F.softmax(self.layer3(x)) # To check with the loss function\n","        return x"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TscRUrEeUOvz","colab_type":"text"},"source":["Make Sure Notebook using GPU (To Enable: Edit->Notebook Setting)"]},{"cell_type":"code","metadata":{"id":"dsltwOfKTUvi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1594228142943,"user_tz":240,"elapsed":243,"user":{"displayName":"Lex Workshop","photoUrl":"","userId":"01577604359645715308"}},"outputId":"fec16960-e487-408c-c938-8a383aa33069"},"source":["use_cuda = torch.cuda.is_available()\n","current_device = torch.cuda.current_device()\n","print(use_cuda)\n","print(torch.cuda.get_device_name(current_device))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["True\n","Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0ytFUgZAUeQI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1594228144792,"user_tz":240,"elapsed":192,"user":{"displayName":"Lex Workshop","photoUrl":"","userId":"01577604359645715308"}},"outputId":"bfb1d7dc-da8e-43a7-ce56-483a2f377596"},"source":["model = MLP(x_train.shape[1])\n","model.to(current_device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","loss_fn = nn.CrossEntropyLoss()\n","epochs = 100\n","\n","def print_(loss):\n","    print (\"The loss calculated: \", loss)\n","\n","print(model)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["MLP(\n","  (layer1): Linear(in_features=4, out_features=50, bias=True)\n","  (layer2): Linear(in_features=50, out_features=20, bias=True)\n","  (layer3): Linear(in_features=20, out_features=3, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vubMTY7zVtGy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594228320513,"user_tz":240,"elapsed":591,"user":{"displayName":"Lex Workshop","photoUrl":"","userId":"01577604359645715308"}},"outputId":"d997be92-7271-4195-b9c8-35abb90f289f"},"source":["x, y = Variable(torch.from_numpy(x_train)).float(), Variable(torch.from_numpy(y_train)).long()\n","x = x.to(current_device)\n","y = y.to(current_device)\n","\n","for epoch in range(1, epochs+1):\n","    print (\"Epoch #\",epoch)\n","    _y = model(x)\n","    loss = loss_fn(_y, y)\n","    print_(loss.item())\n","    \n","    # Zero gradients\n","    optimizer.zero_grad()\n","    loss.backward() # Gradients\n","    optimizer.step() # Update"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Epoch # 1\n","The loss calculated:  0.5752822756767273\n","Epoch # 2\n","The loss calculated:  0.5752279162406921\n","Epoch # 3\n","The loss calculated:  0.5751791596412659\n","Epoch # 4\n","The loss calculated:  0.5751288533210754\n","Epoch # 5\n","The loss calculated:  0.5750843286514282\n","Epoch # 6\n","The loss calculated:  0.5750545859336853\n","Epoch # 7\n","The loss calculated:  0.575009822845459\n","Epoch # 8\n","The loss calculated:  0.5749460458755493\n","Epoch # 9\n","The loss calculated:  0.5749149918556213\n","Epoch # 10\n","The loss calculated:  0.5748721361160278\n","Epoch # 11\n","The loss calculated:  0.5748507380485535\n","Epoch # 12\n","The loss calculated:  0.5748267769813538\n","Epoch # 13\n","The loss calculated:  0.574762761592865\n","Epoch # 14\n","The loss calculated:  0.5747048258781433\n","Epoch # 15\n","The loss calculated:  0.574677586555481\n","Epoch # 16\n","The loss calculated:  0.5746492743492126\n","Epoch # 17\n","The loss calculated:  0.5745875239372253\n","Epoch # 18\n","The loss calculated:  0.5745336413383484\n","Epoch # 19\n","The loss calculated:  0.5744724869728088\n","Epoch # 20\n","The loss calculated:  0.5744338631629944\n","Epoch # 21\n","The loss calculated:  0.5744023323059082\n","Epoch # 22\n","The loss calculated:  0.5743724703788757\n","Epoch # 23\n","The loss calculated:  0.5743376612663269\n","Epoch # 24\n","The loss calculated:  0.5742969512939453\n","Epoch # 25\n","The loss calculated:  0.5742526054382324\n","Epoch # 26\n","The loss calculated:  0.5742077827453613\n","Epoch # 27\n","The loss calculated:  0.5741640329360962\n","Epoch # 28\n","The loss calculated:  0.5741215944290161\n","Epoch # 29\n","The loss calculated:  0.5740979313850403\n","Epoch # 30\n","The loss calculated:  0.5740558505058289\n","Epoch # 31\n","The loss calculated:  0.5740333199501038\n","Epoch # 32\n","The loss calculated:  0.5740060210227966\n","Epoch # 33\n","The loss calculated:  0.5739743113517761\n","Epoch # 34\n","The loss calculated:  0.5739352107048035\n","Epoch # 35\n","The loss calculated:  0.5739065408706665\n","Epoch # 36\n","The loss calculated:  0.573875904083252\n","Epoch # 37\n","The loss calculated:  0.5738581418991089\n","Epoch # 38\n","The loss calculated:  0.5738361477851868\n","Epoch # 39\n","The loss calculated:  0.5738171339035034\n","Epoch # 40\n","The loss calculated:  0.5737934708595276\n","Epoch # 41\n","The loss calculated:  0.5737734436988831\n","Epoch # 42\n","The loss calculated:  0.573746383190155\n","Epoch # 43\n","The loss calculated:  0.573719322681427\n","Epoch # 44\n","The loss calculated:  0.5736798644065857\n","Epoch # 45\n","The loss calculated:  0.5736356377601624\n","Epoch # 46\n","The loss calculated:  0.5735790133476257\n","Epoch # 47\n","The loss calculated:  0.5735184550285339\n","Epoch # 48\n","The loss calculated:  0.5734543204307556\n","Epoch # 49\n","The loss calculated:  0.573392927646637\n","Epoch # 50\n","The loss calculated:  0.5733360648155212\n","Epoch # 51\n","The loss calculated:  0.5732846856117249\n","Epoch # 52\n","The loss calculated:  0.5732381939888\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch # 53\n","The loss calculated:  0.5731959939002991\n","Epoch # 54\n","The loss calculated:  0.573156476020813\n","Epoch # 55\n","The loss calculated:  0.5731188058853149\n","Epoch # 56\n","The loss calculated:  0.573082447052002\n","Epoch # 57\n","The loss calculated:  0.5730467438697815\n","Epoch # 58\n","The loss calculated:  0.5730118155479431\n","Epoch # 59\n","The loss calculated:  0.5729774832725525\n","Epoch # 60\n","The loss calculated:  0.5729442834854126\n","Epoch # 61\n","The loss calculated:  0.5729132890701294\n","Epoch # 62\n","The loss calculated:  0.5728912353515625\n","Epoch # 63\n","The loss calculated:  0.5729493498802185\n","Epoch # 64\n","The loss calculated:  0.5731493830680847\n","Epoch # 65\n","The loss calculated:  0.5735843777656555\n","Epoch # 66\n","The loss calculated:  0.5742639303207397\n","Epoch # 67\n","The loss calculated:  0.5742253065109253\n","Epoch # 68\n","The loss calculated:  0.5735307335853577\n","Epoch # 69\n","The loss calculated:  0.5727334022521973\n","Epoch # 70\n","The loss calculated:  0.5727906227111816\n","Epoch # 71\n","The loss calculated:  0.5733738541603088\n","Epoch # 72\n","The loss calculated:  0.573346734046936\n","Epoch # 73\n","The loss calculated:  0.57284015417099\n","Epoch # 74\n","The loss calculated:  0.5725187063217163\n","Epoch # 75\n","The loss calculated:  0.572781503200531\n","Epoch # 76\n","The loss calculated:  0.5730040669441223\n","Epoch # 77\n","The loss calculated:  0.5726733803749084\n","Epoch # 78\n","The loss calculated:  0.5724200010299683\n","Epoch # 79\n","The loss calculated:  0.5725669264793396\n","Epoch # 80\n","The loss calculated:  0.572689950466156\n","Epoch # 81\n","The loss calculated:  0.5725229978561401\n","Epoch # 82\n","The loss calculated:  0.5723214149475098\n","Epoch # 83\n","The loss calculated:  0.572394847869873\n","Epoch # 84\n","The loss calculated:  0.5725022554397583\n","Epoch # 85\n","The loss calculated:  0.5723616480827332\n","Epoch # 86\n","The loss calculated:  0.5722190141677856\n","Epoch # 87\n","The loss calculated:  0.572270393371582\n","Epoch # 88\n","The loss calculated:  0.5723604559898376\n","Epoch # 89\n","The loss calculated:  0.5722928047180176\n","Epoch # 90\n","The loss calculated:  0.5721394419670105\n","Epoch # 91\n","The loss calculated:  0.5721468925476074\n","Epoch # 92\n","The loss calculated:  0.5722212791442871\n","Epoch # 93\n","The loss calculated:  0.5721589922904968\n","Epoch # 94\n","The loss calculated:  0.5720509886741638\n","Epoch # 95\n","The loss calculated:  0.5720373392105103\n","Epoch # 96\n","The loss calculated:  0.5720751881599426\n","Epoch # 97\n","The loss calculated:  0.5720474123954773\n","Epoch # 98\n","The loss calculated:  0.5719631314277649\n","Epoch # 99\n","The loss calculated:  0.5719389915466309\n","Epoch # 100\n","The loss calculated:  0.5719286203384399\n"],"name":"stdout"}]}]}